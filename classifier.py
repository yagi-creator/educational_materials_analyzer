"""
ğŸ”§ å•†å“ååˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¾Œã‹ã‚‰èª¿æ•´ãƒ»æ‹¡å¼µå¯èƒ½ï¼‰

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯ä»¥ä¸‹ã®åˆ†é¡ã‚’è¡Œã„ã¾ã™ï¼š
- å­¦å¹´åˆ¤åˆ¥ï¼ˆå°1-6, ä¸­1-3, é«˜æ ¡ï¼‰
- ç§‘ç›®åˆ¤åˆ¥ï¼ˆå›½èª, ç®—æ•°, æ•°å­¦, è‹±èª, ç†ç§‘, ç¤¾ä¼šï¼‰
- å­£ç¯€åˆ¤åˆ¥ï¼ˆæ˜¥æœŸ, å¤æœŸ, å†¬æœŸï¼‰
- å…¥è©¦æ•™æåˆ¤åˆ¥
- åˆæœ¬æ•™æåˆ¤åˆ¥
"""

import re
import unicodedata
import pandas as pd

# å›ºå®šè¨­å®š
UNIT_PRICE = 1500  # å£²ä¸Šå¢—è¦‹è¾¼è¨ˆç®—ç”¨å˜ä¾¡

# ğŸ”§ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ï¼ˆå¾Œã‹ã‚‰èª¿æ•´å¯èƒ½ï¼‰
SEASON_KEYWORDS = {
    "æ˜¥æœŸ": [
        "spring", "Spring", "SPRING",
        "ã‚¹ãƒ—ãƒªãƒ³ã‚°", "ï½½ï¾Œï¾Ÿï¾˜ï¾ï½¸ï¾", "ã‚¹ãƒ—ãƒªãƒ³ã‚¯", "ï½½ï¾Œï¾Ÿï¾˜ï¾ï½¸",
        "æ˜¥æœŸ", "æ˜¥å­£", "æ˜¥è¬›", "æ–°å­¦æœŸ"
    ],
    "å¤æœŸ": [
        "summer", "Summer", "SUMMER",
        "ã‚µãƒãƒ¼", "ï½»ï¾ï½°", "ã‚µãƒ", "ï½»ï¾",
        "å¤æœŸ", "å¤å­£", "å¤è¬›", "å¤ä¼‘ã¿"
    ],
    "å†¬æœŸ": [
        "winter", "Winter", "WINTER",
        "ã‚¦ã‚£ãƒ³ã‚¿ãƒ¼", "ã‚¦ã‚¤ãƒ³ã‚¿ãƒ¼", "ï½³ï½¨ï¾ï¾€ï½°", "ï½³ï½²ï¾ï¾€ï½°",
        "ã‚¦ã‚£ãƒ³ã‚¿", "ã‚¦ã‚¤ãƒ³ã‚¿", "ï½³ï½¨ï¾ï¾€", "ï½³ï½²ï¾ï¾€",
        "ã‚¦ã‚£ãƒ³ã‚¿ï½°", "ã‚¦ã‚¤ãƒ³ã‚¿ï½°", "ï½³ï½¨ï¾ï¾€ï½°", "ï½³ï½²ï¾ï¾€ï½°",
        "å†¬æœŸ", "å†¬å­£", "å†¬è¬›", "å†¬ä¼‘ã¿"
    ]
}

EXAM_KEYWORDS = [
    "å…¥è©¦", "å—é¨“", "é«˜æ ¡å…¥è©¦", "å…¥è©¦å¯¾ç­–", "å—é¨“å¯¾ç­–",
    "éå»å•", "ç›´å‰å¯¾ç­–", "åˆæ ¼", "å¿—æœ›æ ¡"
]

COMPOSITE_KEYWORDS = [
    "åˆæœ¬", "ç·åˆ", "ã‚»ãƒƒãƒˆ", "ãƒ‘ãƒƒã‚¯", "ã¾ã¨ã‚",
    "å…¨ç§‘ç›®", "5ç§‘ç›®", "3ç§‘ç›®", "ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³",
    "è¤‡åˆ", "ç·å¾©ç¿’", "ç·ã¾ã¨ã‚", "çµ±åˆ"
]

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–"""
    if pd.isna(text):
        return ""
    text = str(text).strip()
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'[ãƒ¼ï½°âˆ’â€•â€]', 'ãƒ¼', text)
    text = re.sub(r'\s+', ' ', text)
    return text

def extract_grade(product_name):
    """å­¦å¹´æŠ½å‡º"""
    normalized = normalize_text(product_name)
    
    # å°å­¦ç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = [
        r'å°\s*([1-6ï¼‘-ï¼–])',
        r'å°å­¦\s*([1-6ï¼‘-ï¼–])\s*å¹´',
        r'([1-6ï¼‘-ï¼–])\s*å¹´ç”Ÿ'
    ]
    for pattern in patterns:
        match = re.search(pattern, normalized)
        if match:
            num = match.group(1)
            num = num.translate(str.maketrans('ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–', '123456'))
            return f'å°{num}'
    
    # ä¸­å­¦ç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = [
        r'ä¸­\s*([1-3ï¼‘-ï¼“])',
        r'ä¸­å­¦\s*([1-3ï¼‘-ï¼“])\s*å¹´'
    ]
    for pattern in patterns:
        match = re.search(pattern, normalized)
        if match:
            num = match.group(1)
            num = num.translate(str.maketrans('ï¼‘ï¼’ï¼“', '123'))
            return f'ä¸­{num}'
    
    # é«˜æ ¡ãƒ‘ã‚¿ãƒ¼ãƒ³
    if re.search(r'(é«˜æ ¡|é«˜\s*[1-3ï¼‘-ï¼“]|é«˜ç­‰å­¦æ ¡)', normalized):
        return 'é«˜æ ¡'
    
    return None

def extract_subject(product_name, grade=None):
    """ç§‘ç›®æŠ½å‡º"""
    normalized = normalize_text(product_name)
    
    subject_patterns = {
        'å›½èª': [r'å›½èª', r'ç¾ä»£æ–‡', r'å¤æ–‡', r'æ¼¢æ–‡', r'å›½'],
        'ç®—æ•°': [r'ç®—æ•°', r'ç®—'],
        'æ•°å­¦': [r'æ•°å­¦', r'æ•°'],
        'è‹±èª': [r'è‹±èª', r'è‹±'],
        'ç†ç§‘': [r'ç†ç§‘', r'ç†', r'ç‰©ç†', r'åŒ–å­¦', r'ç”Ÿç‰©', r'åœ°å­¦'],
        'ç¤¾ä¼š': [r'ç¤¾ä¼š', r'ç¤¾', r'æ­´å²', r'åœ°ç†', r'å…¬æ°‘', r'æ”¿æ²»', r'çµŒæ¸ˆ']
    }
    
    found_subjects = []
    for subject, patterns in subject_patterns.items():
        for pattern in patterns:
            if re.search(pattern, normalized):
                found_subjects.append(subject)
                break
    
    if not found_subjects:
        return 'ãã®ä»–'
    
    # å­¦å¹´ã‚’è€ƒæ…®ã—ãŸèª¿æ•´
    if grade and grade.startswith('ä¸­'):
        if 'æ•°å­¦' in found_subjects:
            return 'æ•°å­¦'
        elif 'ç®—æ•°' in found_subjects:
            return 'æ•°å­¦'
    elif grade and grade.startswith('å°'):
        if 'ç®—æ•°' in found_subjects:
            return 'ç®—æ•°'
        elif 'æ•°å­¦' in found_subjects:
            return 'ç®—æ•°'
    
    return found_subjects[0]

def extract_season_and_exam(product_name):
    """å­£ç¯€ãƒ»å…¥è©¦åˆ¤åˆ¥"""
    normalized = normalize_text(product_name)
    
    # å…¥è©¦åˆ¤åˆ¥ï¼ˆæœ€å„ªå…ˆï¼‰
    for keyword in EXAM_KEYWORDS:
        if keyword in normalized:
            return None, True
    
    # å­£ç¯€åˆ¤åˆ¥
    for season, keywords in SEASON_KEYWORDS.items():
        for keyword in keywords:
            if keyword in normalized:
                return season, False
    
    return None, False

def is_composite_material(product_name):
    """åˆæœ¬æ•™æåˆ¤åˆ¥"""
    normalized = normalize_text(product_name)
    for keyword in COMPOSITE_KEYWORDS:
        if keyword in normalized:
            return True
    return False

def classify_product_comprehensive(product_name):
    """å•†å“åç·åˆåˆ†é¡"""
    if pd.isna(product_name):
        return {
            'å­¦å¹´': None,
            'ç§‘ç›®': 'ãã®ä»–',
            'å­£ç¯€': None,
            'å…¥è©¦ãƒ•ãƒ©ã‚°': False,
            'åˆæœ¬ãƒ•ãƒ©ã‚°': False,
            'ã‚«ãƒ†ã‚´ãƒª': 'é€šå¹´'
        }
    
    grade = extract_grade(product_name)
    subject = extract_subject(product_name, grade)
    season, is_exam = extract_season_and_exam(product_name)
    is_composite = is_composite_material(product_name)
    
    # ã‚«ãƒ†ã‚´ãƒªæ±ºå®š
    if is_exam:
        category = 'å…¥è©¦'
    elif season:
        category = season
    else:
        category = 'é€šå¹´'
    
    return {
        'å­¦å¹´': grade,
        'ç§‘ç›®': subject,
        'å­£ç¯€': season,
        'å…¥è©¦ãƒ•ãƒ©ã‚°': is_exam,
        'åˆæœ¬ãƒ•ãƒ©ã‚°': is_composite,
        'ã‚«ãƒ†ã‚´ãƒª': category
    }
